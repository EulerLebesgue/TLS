# 機械学習
## 線形回帰モデル
### まとめ
- 回帰問題を解くための教師あり学習
- 与えられたデータを一次式で予測する。  
基本的には説明変数がn次元、目的変数が１次元する。  
出力を多次元にすることも可能  
誤差項は偶発誤差だけでなく、隠れた説明変数の項が乗っていることがある。
目的変数に対して、目的変数が少ないとき。
基本的に目的変数の次元より、重みの次元は低い
- 誤差(最小二乗誤差)を最小とするのが目的
- - ただし外れ値に弱い。影響されやすい



バクニックの原理
みつどひすいてい
データ分割　汎化性能を計測するために必要。数学的にも計算できる
Huber損失　Tukey損失
射影行列
一般化逆行列

## 非線形回帰モデル
### まとめ
- 非線形であるのは入力パラメータについて、重みについては線形。
- 線形回帰と異なるのは、入力を基底関数で射影した後に線形結合を行う点
- 学習後の重みの計算も線形回帰と本質は変わらない。
- 基底関数の影響が大きくなりすぎて、過学習を起こす可能性がある。
- 過学習の抑制方法として正則化により、重みパラメータの値を制限する。

最適化　KKT条件
## ロジスティック回帰モデル
- 回帰とついているが分類モデルの一つ。
- 回帰には識別的アプローチ(直接確率を計算)・生成的アプローチ(ベイズの定理からを間接的に確率を計算)がある。ロジスティック回帰は識別的アプローチ
- 出力値は0か1の値とする。
- 線形結合で得た値を、出力が0から1の確率として捉えられるように関数をかませる。例えばシグモイド関数
- 尤度関数とする重みを探す。
- 桁落ちが起こる可能性があるため、実装では尤度関数を対数をとり計算する。
- 
### まとめ
## 主成分分析
### まとめ
- 次元を圧縮する。その際にデータの散らばりが残る(分散が最大化する)ような散らばり方を選択する。
## アルゴリズム
### まとめ
## SVM
### まとめ


