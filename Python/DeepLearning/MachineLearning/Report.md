# 機械学習
## 線形回帰モデル
- 回帰問題を解くための教師あり学習
- 与えられたデータを一次式で予測する。  
基本的には説明変数がn次元、目的変数が１次元する。  
出力を多次元にすることも可能  
誤差項は偶発誤差だけでなく、隠れた説明変数の項が乗っていることがある。
目的変数に対して、目的変数が少ないとき。
基本的に目的変数の次元より、重みの次元は低い
- 誤差(最小二乗誤差)を最小とするのが目的
- - ただし外れ値に弱い。影響されやすい



バクニックの原理
みつどひすいてい
データ分割　汎化性能を計測するために必要。数学的にも計算できる
Huber損失　Tukey損失
射影行列
一般化逆行列

## 非線形回帰モデル
- 非線形であるのは入力パラメータについて、重みについては線形。
- 線形回帰と異なるのは、入力を基底関数で射影した後に線形結合を行う点
- 学習後の重みの計算も線形回帰と本質は変わらない。
- 基底関数の影響が大きくなりすぎて、過学習を起こす可能性がある。
- 過学習の抑制方法として正則化により、重みパラメータの値を制限する。

最適化　KKT条件
## ロジスティック回帰モデル
- 回帰とついているが分類モデルの一つ。
- 回帰には識別的アプローチ(直接確率を計算)・生成的アプローチ(ベイズの定理からを間接的に確率を計算)がある。ロジスティック回帰は識別的アプローチ
- 出力値は0か1の値とする。
- 線形結合で得た値を、出力が0から1の確率として捉えられるように関数をかませる。例えばシグモイド関数
- 尤度関数とする重みを探す。
- 桁落ちが起こる可能性があるため、実装では尤度関数を対数をとり計算する。
### まとめ
## 主成分分析
多変量データの持つ構造をより少数個の指標に圧縮することが目標(次元圧縮)
データの散らばりが残る(分散が最大化する)ような散らばり方を選択する。
ラグランジュ未定乗数法により、分散共分散行列の固有値と固有ベクトルが求める。
### 寄与率
元のデータの分散は、すべての主成分の総和に等しい。
- 寄与率：第ｋ主成分の分散が主成分の総分散にどれほどの割合を占めるのかを表したもの
- 累積寄与率：第1～第ｋ主成分の分散が主成分の総分散にどれほどの割合を占めるのかを表したもの
### まとめ
## アルゴリズム
### kNN法
教師あり学習。分類のタスク。ある点から近い順にk個の点を探し、そのk個の点が属している集合のうち最も多いものを割り当てる。(多数決)
### k-means法
教師なし学習。クラスタリング。分類したいクラスと初期値(クラスタの中心)を決めて、近い中心のクラスを割り当てる。その後中心を再計算するという手続きを繰り返す手法。

どちらの手法も距離計算が入るので、計算コストが高い。
## SVM
- 教師あり学習。2値分類問題に適用。
### ハードSVM
与えられた集合を線形分離する超平面(2次元だと直線)を探す。この時データとのマージンが最大となる直線を探す。
### ソフトSVM
線形分離不能な問題へ対応するため、いくつかの誤差を許容する。そのために最適化問題にスラック変数を導入する。  
ペナルティ項の係数が誤差の許容度を表す。係数が大きいほど誤差を許さない。
### カーネル法
与えられた集合を線形分離する超局面(2次元だと曲線)で分離する。  
高次元空間に射影した場合、計算コストが上がるが、変換の選び方を工夫すれば変換の内積の関数のみ考慮すればよい(カーネルトリック)
有名なカーネル関数
- RBF関数(動径基底関数)：ガウシアン
- シグモイドカーネル
- 多項式カーネル
- 指数カーネル
- 線形カーネル(単なる内積)



