# 機械学習
##Chapter1 線形回帰モデル
- 回帰問題を解くための教師あり学習
- 与えられたデータを一次式で予測する。  
基本的には説明変数がn次元、目的変数が１次元する。  
出力を多次元にすることも可能  
誤差項は偶発誤差だけでなく、隠れた説明変数の項が乗っていることがある。
目的変数に対して、目的変数が少ないとき。
基本的に目的変数の次元より、重みの次元は低い
- 誤差(最小二乗誤差)を最小とするのが目的
- - ただし外れ値に弱い。影響されやすい

### 補足 決定係数
- 推定された回帰式の当てはまりの良さ（度合い）を表す。
- 0から1までの値をとる
- 1に近いほど、回帰式が実際のデータに当てはまっていることを表しており、説明変数が目的変数をよく説明していると言える。

### ハンズオン
テーマ：bostonの住宅価格モデル  
sklearnを使うと簡単に回帰・分類・クラスタリングで同様のインターフェイスを使うことができる。
単線形回帰も重線形回帰も下記で予測できる。

``
model = LinearRegression()
model.fit(data, target)
model.predict([[7]])
``

単線形回帰と重線形回帰を比較した場合、重線形回帰の方が精度が若干良い
```
print('単回帰決定係数: %.3f, 重回帰決定係数 : %.3f' % (model.score(data,target), model2.score(data2,target2)))
```
単回帰決定係数: 0.483, 重回帰決定係数 : 0.542 

##Chapter2 非線形回帰モデル
- 非線形であるのは入力パラメータについて、重みについては線形。
- 線形回帰と異なるのは、入力を基底関数で射影した後に線形結合を行う点
### よく使われる基底関数
- 多項式関数(x^j)
- ガウス型基底関数(exp{(x-x_ave}^2/2h_i}
- スプライン関数/Bスプライン関数
- 学習後の重みの計算も線形回帰と本質は変わらない。
- 基底関数の影響が大きくなりすぎて、過学習を起こす可能性がある
### 未学習と過学習
- 未学習：学習誤差に対して、十分小さなモデルが得られない状態
- 過学習：学習誤差は小さいが、テスト集合誤差との差が大きい状態
ここでいう誤差は3つある。
- バリアンス：訓練データの選び方による誤差
- バイアス：モデルの表現力が不足していることによる誤差。
- ノイズ：データの測定誤差によるもの
#### バイアスとバリアンスのトレードオフ
単純なモデルではバイアスが低く、バリアンスが大きくなる。一方で複雑なモデルだとバイアスが大きく・バリアンスが小さい

過学習
- 学習データの数を増やす(たくさん学習させる)
- 不要な基底関数を削除(表現力の抑止)
- 正則化(罰則項)により、重みパラメータの値を制限する。(表現力の抑止)

### 正則化の種類
- Rigde正規化：L2ノルムの罰則項を追加。パラメータを0に近づけるような推定を行う
- Lasso正規化：L1ノルムの罰則項を追加。いくつかのパラメータを0にする。(スパース化)

### モデルの汎化性能評価
- ホールドアウト法：データを学習用と検証用の２つに分けて一気に学習する。データ数が多い時には簡単。ただしデータ数が少ない時には使うべきではない。
- 交差検証法(クロスバリデーション)：データをｋ個に重複を許さず分割して、k-1個を学習用、1個を検証用に使う。これをk回繰り返す。このk回の学習の平均(CV値)で精度を見る
- ホールドアウト法とクロスバリデーションであれば、後者の方が精度が良い。たとえ前者の数値がよかったとしても後者の方が汎化性能が高い場合がある。

## Chapter3 ロジスティック回帰モデル
- 回帰とついているが分類モデルの一つ。
- 回帰には識別的アプローチ(直接確率を計算)・生成的アプローチ(ベイズの定理からを間接的に確率を計算)がある。ロジスティック回帰は識別的アプローチ
- 出力値は0か1の値とする。
- 線形結合で得た値を、出力が0から1の確率として捉えられるように関数をかませる。例えばシグモイド関数
- 尤度関数とする重みを探す。
- 桁落ちが起こる可能性があるため、実装では尤度関数を対数をとり計算する。
### まとめ
##Chapter4 主成分分析
多変量データの持つ構造をより少数個の指標に圧縮することが目標(次元圧縮)
データの散らばりが残る(分散が最大化する)ような散らばり方を選択する。
ラグランジュ未定乗数法により、分散共分散行列の固有値と固有ベクトルが求める。
### 寄与率
元のデータの分散は、すべての主成分の総和に等しい。
- 寄与率：第ｋ主成分の分散が主成分の総分散にどれほどの割合を占めるのかを表したもの
- 累積寄与率：第1～第ｋ主成分の分散が主成分の総分散にどれほどの割合を占めるのかを表したもの
### まとめ

##Chapter4 アルゴリズム
### kNN法
教師あり学習。分類のタスク。ある点から近い順にk個の点を探し、そのk個の点が属している集合のうち最も多いものを割り当てる。(多数決)
### k-means法
教師なし学習。クラスタリング。分類したいクラスと初期値(クラスタの中心)を決めて、近い中心のクラスを割り当てる。その後中心を再計算するという手続きを繰り返す手法。
- 初期値(中心点)の位置により結果が大きく変わる。その解消のためk-means++法というのがある。
上記2つはどちらの手法も距離計算が入るので、計算コストが高い。

##Chapter5 SVM
- 教師あり学習。2値分類問題に適用。
### ハードSVM
- 与えられた集合を線形分離する超平面(2次元だと直線)を探す。この時データとのマージンが最大となる直線を探す。
- 決定境界に最も近い点(サポートベクター)以外は予測に影響を与えない。
- 上記からサポートベクターは2本存在する
### ソフトSVM
線形分離不能な問題へ対応するため、いくつかの誤差を許容する。そのために最適化問題にスラック変数を導入する。  
ペナルティ項の係数が誤差の許容度を表す。係数が大きいほど誤差を許さない。
### カーネル法
与えられた集合を線形分離する超局面(2次元だと曲線)で分離する。  
高次元空間に射影した場合、計算コストが上がるが、変換の選び方を工夫すれば変換の内積の関数のみ考慮すればよい(カーネルトリック)
有名なカーネル関数
- RBF関数(動径基底関数)：ガウシアン
- シグモイドカーネル
- 多項式カーネル
- 指数カーネル
- 線形カーネル(単なる内積)



