# 深層学習day1
- ディープラーニングの目的はプログラムでモデルを明示するのではなく、中間層を重ね重みとバイアスを学習させることで自動的にモデルを作成する。
## Section1 入力層～中間層
- 入力層と中間層で一つのユニットと考える。
- 中間層は何層でも重ねることができる。
- 数字で置き換えられるものはどのようなものでも入力として扱うことができる。
- 入力と重みとバイアスの線形結合はPythonでnp.arrayとnp.dotで簡単に実装できる。まんま一次関数の直線式と変わらない。
### 確認テスト
中間層入力
```
u = np.dot(x, W) + b
```
中間層出力(活性化関数はReLu関数)
```
z = functions.relu(u)
```
### 実装
パラメータの初期化の種類
```
#0でしきつめ
W = np.zeros(2)
#1 でしきつめ
W = np.ones(2)
#0-1の値で敷き詰め(小数)
W = np.random.rand(2)
# 5までの整数でしきつめ
W = np.random.randint(5, size=(2))
# -5から5までの範囲で出力　random.rand()で0-1の乱数を生成 
b = np.random.rand() * 10 -5
```
ReLuじゃなくて、シグモイドを使うことも可能。もちろん出力は0-1
```
z = functions.sigmoid(u)
```
結果：0.9484770701710731
## Section2 活性化関数
- 線形の定義は加法性と斉次性を満たすこと。
- 活性化関数で、次の入力の強弱を設定することができる。
- 中間層用の活性化関数は以下
- - ステップ関数　最近はあまり使われていない。線形分離可能な問いしか使えない。
- - シグモイド関数　ステップ関数の連続版のようなもの。勾配消失問題を起こしやすい。
- - ReLU関数　0より大きい入力に対して、恒等写像を返す。勾配消失とスパース化に貢献。
### 確認テスト
順伝播（単層・複数ユニット）中間層の出力箇所
```
z = functions.sigmoid(u)
```
### 実装
それぞれの活性化関数の定義。ReLuは場合分けと使わなくても最大値で記述できる。
```
### シグモイド関数
def sigmoid(x):
    return 1/(1 + np.exp(-x))

# ReLU関数
def relu(x):
    return np.maximum(0, x)

# ステップ関数（閾値0）
def step_function(x):
    return np.where( x > 0, 1, 0) 
```
重みが配列となっても初期化可能。ベクトルと大差ない
```
#W = np.zeros((4,3))
W = np.ones((4,3))
W = np.random.rand(4,3)
W = np.random.randint(5, size=(4,3))
```
## Section3 出力層
### 中間層と出力層の出力の役割について
- 中間層：閾値の前後で、出力の強弱を調整。
- 出力層：出力の比率はそのままに変換。分類問題においては総和が１となるように変換する。
- 出力層の活性化関数は以下
- - ソフトマックス関数：多クラス分類問題で利用。誤差関数は交差エントロピーを利用
- - 恒等写像：回帰問題で利用。誤差関数は二乗和誤差
- - シグモイド関数：２値分類問題で利用。誤差関数は交差エントロピーを利用
### 確認テスト
2乗和誤差について
- 二乗和誤差でないと、異符号の誤差が打ち消しあって誤差を表すことができないために、二乗する。係数の1/2は微分計算のためのものであり、本質的ではない。
ソフトマックス関数について
- 出力は配列。np.exp(x)はスカラーではなくベクトル値。np.sum(np.exp(x))は総和なのでスカラー
交差エントロピーについて
- 真数部分が０とならないように微小な値を加えておく。  
- 総和の形式だが、d_iは正解のラベルのみ１・その他は０の場合(One-Hot-Vector)、実質的に返すのは正解についての対数値。

###
実装箇所
```
def softmax(x):
    if x.ndim == 2:
        x = x.T
        x = x - np.max(x, axis=0)
        y = np.exp(x) / np.sum(np.exp(x), axis=0)
        return y.T

    x = x - np.max(x) # オーバーフロー対策
```

## Section4 勾配降下法
深層学習の目的は、精度を高めるパラメータを求めること。そのため結果から、誤差の分だけパラメータを修正する。  
学習の際には学習率によって、学習の精度が大きく変わる。
- 学習率が大きすぎる場合：解にたどり着かず、発散する。
- 学習率が小さすぎる場合：学習が遅い。また局所極小解に捕らわれやすい。
入力 - 出力 - 誤差の計算 - パラメータの更新という一つの流れをエポックという。
### 勾配降下法
全サンプルの平均誤差を計算する。
### 確率的勾配降下法
ランダムに抽出したサンプルの誤差を計算。利点は次の三点
- データが冗長な場合の計算コストの削減
- 鞍点への収束を回避しやすい
- オンライン学習が可能。
### ミニバッチ勾配降下法
ランダムに分割したデータの集合に属するサンプルの平均誤差。利点は次  
確率的勾配降下法のメリットを損なわず、計算機の計算資源を有効利用できる。このため、CPUを利用したスレッド並列化やGPUを利用したSIMD並列化

### 確認テスト(オンライン学習とは)
データをすべて学習させるのではなく、その都度学習させる。そのため、すべてのデータをそろえる必要がない。
## Section5 誤差逆伝搬
## 誤差の計算
数値微分(パラメータに微小な値の加えたものと引いたものの差をとって、微小量の２倍で割る。)を用いる。  
しかし入力層から出力層に向けての計算をすべての層の重みに対して行うので、計算量が多くなる。(純伝搬)  
そのため、誤差逆伝搬法を使う。
## 誤差逆伝搬とは
産出された誤差を、出力層側から順に微分し、入力層に向かって伝搬していく。各パラメータの微分値を解析的に計算する。そのため、不要な再帰計算を避けることができる。  
実現するために微分の連鎖律を用いる。
### 計算速度
処理速度と値段は次の関係。CPU < GPU <FPGA < ASIC(TPU)TPUはクラウドのものを間借りする。
### 入力層の設計
入力層としてとるべきでないデータ
- 欠損値が多いデータ
- 誤差の大きいデータ
- 出力を加工した情報　生のデータから出力を得るニューラルネットワークをエンドtoエンドという。
- 連続性のないデータ
- 無意味な数が割り当てられているデータ
### 欠損値の取り扱い
- ゼロ詰め
- 欠損値を含む集合を除外
- 入力として採用しない。
### 正規化と正則化
正規化は入力を0-1に収める。(入力の最大値で割る。)  
正則化は平均0、分散１のガウス分布に収める。
# 深層学習day2
## Section1 勾配消失問題
誤差逆伝搬法が階層に進んでいくにつれて、勾配がどんどん緩やかになる。そのため学習によるパラメータ更新が進まなくなる。  
学習しても正解率が上がらない、連鎖律で伝わる結果が0-1の間をとるため、積をとると値が小さくなる。
### 対策1　活性化関数の工夫
- ReLu関数：0以上の時、y=x,0未満の場合は0を返す関数。勾配消失問題およびスパース化に貢献する。
- - この関数により、重み更新に貢献しない重みの更新を行わない。
### 対策2　初期値の設定方法
重みの初期値は乱数によって設定する。
- Xavierの初期値：正規分布をとった後、前のlayerのノードの平方根で割る。S字カーブの活性化関数に対して有効
np.randam.rand(input_layer_size,hidden_layer_size) / np.sqrt(input_layer_size)  
np.random.rand()は標準正規分布に沿った乱数を返す。変数の指定で出力される行列のサイズを決定する。

比較
- 正規分布による初期化：出力値が0または1に集中する。勾配消失が起きやすい
- 正規分布の標準偏差を極端(0.01)に小さくする。出力地が0.5に集中する。表現が失われ学習できていない。
- Heの初期値：正規分布の重みに2/ｎの平方根をかける。nは前層のノード数。ReLu関数に対して有効
np.randam.rand(input_layer_size,hidden_layer_size) / np.sqrt(input_layer_size)* np.sqrt(2)
比較
- 正規分布による初期化：出力値が0に集中する。
- 正規分布の標準偏差を極端(0.01)に小さくする。出力値が0に集中する。

### 対策3　バッチ正規化
ミニバッチ単位で、入力値のデータの偏りを抑制する手法  
※ミニバッチサイズは処理能力で目安がある。GPUは64枚まで、TPUは256枚まで  
統計の標準的な正規化(入力から平均を引いた後、標準偏差に微小値を加えたもので割る。)をおこなって、パラメータを移動させる。
### 確認テスト
シグモイド関数は勾配消失問題を起こしやすい。(導関数の最大値が0.25(x=0)。シグモイド関数の導関数はシグモイド関数の2次式なので最大値を容易に求められる。)
### 確認テスト
重みをすべて0(同じ値に設定する)と重みの値が均一に更新される。多数の重みをもつ意味がない。

## Section2 学習率最適化手法
初期の学習率を大きく設定し、徐々に学習率を小さくしていく。(学習率を可変させる。)
### モメンタム
前回の重みに慣性量をかけて値から、学習量と誤差の微分値をかけたものを引き、それを現在の重みに加える。  
大域的局所解に到達しやすい。極小解に捕らわれると一気に学習が進む。株価の移動平均に近い動きをする。
### AdaGrant
変数に誤差関数の微分値の二乗を保持するものを用意して、それを用いて学習率を更新する。  
勾配の緩やかな斜面に対して、最適値に近づける。  
学習率が徐々に小さくなるので、鞍点問題を引き起こすことがある。
### RMSProp
AdaGrantの式に回の誤差の二乗と、前回までの勾配情報をどの程度使用するかを変える。  
局所的最適解にはならず、大域的最適解となる。  
ハイパーパラメータの調整が必要な場合が少ない。
### Adam
モメンタムとRMSPropのいいとこどりをしたような手法。
## Section3 過学習
### 過学習とは
テスト誤差と訓練誤差とで学習曲線が乖離すること。要因としては
- パラメータが多い
- パラメータの値が適切ではない。
- ノードが多い
自由度が高いためおきるので、対策はネットワークの自由度を抑える。

### L1正則化、L2正則化
コンセプトはWeight decay(荷重減衰)
- 過学習は一部の重みが極端に大きくなることで、特定の入力に過剰に反応してしまうことによる。
- そのため、正則化項（どちらかというと条件か？）を加算することで、重みを抑制する。
### L1正則化(Lasso正則)
罰則項としてL1ノルムを採用している。(ひし形)そのため、最適解を得た場合、重みが0となる箇所があることがある。(スパース推定)
### L2正則化
罰則項としてL1ノルムを採用している。(円)最適解を得た場合、重みが0とならない縮小推定。(スパース推定)
### 例題チャレンジ
L2正則化で勾配更新(微分)を行う。その結果、係数が吸収されてgrad += rate * paramとなる。　　
L1正則化で勾配更新(微分)を行う。結果は定数(-1,0,1)となるのでnumpyのsign(param)(符号関数)を用いる。
### ドロップアウト
ランダムにノードを削除して学習させること。そのため、データ量を変化させずに、異なるモデルを学習させていると解釈できる。
## Section4 畳み込みニューラルネットワークの概念
画像処理によく用いられるネットワーク。次元的なつながり(データが近い箇所で入力が似ている)があるものなら応用が利くもの。
- 単一チャンネル:各次元の入力がスカラーのもの(例(2次元)：音声データのフーリエ変換(時刻、周波数、強度))
- 複数チャンネル：次元の入力がベクトルのものが存在(例(2次元)：カラー画像(x、y、(R,G,B)))
全結合層は今まで学習したニューラルネットワークのこと  
全結合層までは次元の情報を保ったままの出力を得る。(特徴量の抽出機能)  
全結合層からは人間のほしい処理へと変換する作業。
### 畳み込み層
チェンネルに対して、フィルターをずらしながら出力を得る。その後はバイアスを加え活性化関数の処理を行う。  
全結合層では各チャンネルの関連性が学習に反映させられない。  
フィルターにより、画像であれば周辺の情報を取得しながら処理を行うことになるので、次元間の関係を保存できる。  
実装する際には処理を高速化するために、読み込んだ要素を行、または列で並べる。これにより、重みのドット積が簡単に計算できる。
出力画像のサイズの公式があるが、覚えにくいので等差数列の公式等から導くのが安全。
### バイアス
フィルターで処理した情報に定数を加える。ニューラルネットワークのバイアスと同義。
### パディング
出力画像が入力画像と同じサイズになるように、入力を押し広げる
- すべて0で埋める。
- 隣と同じ数字を入れていく。
### ストライド
フィルターが一度に動く量を決める。
### チャンネル
フィルターの数を決定する要素。
### プーリング層
フィルター内で特定の操作を行い出力する。フィルター内の出力を取得(MaxPooling)、平均値を取得(AvgPooling)
## Section5 最新のCNN
### AlexNet
5層の畳み込み層およびプーリング層など、それに続く3層の全結合層から構成される。  
全結合層への処理
- Ftatten そのまま一列に数字を並べる
- Global Max Pooling チャンネルの中で最大値を出力する。  
- Global Avg Pooling チャンネルの中の平均値を出力する。  
サイズ4096の全結合層の出力に過学習を防ぐためドロップアウトを使用している。
### 確認テスト
フィルターサイズの畳み込んだ際の出力は等差数列の公式を持ちいることで暗記をしなくてよい。
