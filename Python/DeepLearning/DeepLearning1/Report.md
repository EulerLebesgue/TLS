# 深層学習day1
- ディープラーニングの目的はプログラムでモデルを明示するのではなく、中間層を重ね重みとバイアスを学習させることで自動的にモデルを作成する。
## Section1 入力層～中間層
- 入力層と中間層で一つのユニットと考える。
- 中間層は何層でも重ねることができる。
- 数字で置き換えられるものはどのようなものでも入力として扱うことができる。
- 入力と重みとバイアスの線形結合はPythonでnp.arrayとnp.dotで簡単に実装できる。まんま一次関数の直線式と変わらない。
## Section2 活性化関数
- 線形の定義は加法性と斉次性を満たすこと。
- 活性化関数で、次の入力の強弱を設定するk十ができる。
- 中間層用の活性化関数は以下
- - ステップ関数　最近はあまり使われていない。線形分離可能な問いしか使えない。
- - シグモイド関数　ステップ関数の連続版のようなもの。勾配消失問題を起こしやすい。
- - ReLU関数　0より大きい入力に対して、恒等写像を返す。勾配消失とスパース化に貢献。
- 出力層の活性化関数は以下
- - ソフトマックス関数
- - 恒等写像
- - シグモイド関数
## Section3 出力層
## Section4 勾配降下法
## Section5 誤差逆伝搬
# 深層学習day2
## Section1 勾配消失問題
## Section2 学習率最適化手法
## Section3 過学習
## Section4 畳み込みニューラルネットワークの概念
## Section5 最新のCNN

