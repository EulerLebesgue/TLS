# 深層学習day1
- ディープラーニングの目的はプログラムでモデルを明示するのではなく、中間層を重ね重みとバイアスを学習させることで自動的にモデルを作成する。
## Section1 入力層～中間層
- 入力層と中間層で一つのユニットと考える。
- 中間層は何層でも重ねることができる。
- 数字で置き換えられるものはどのようなものでも入力として扱うことができる。
- 入力と重みとバイアスの線形結合はPythonでnp.arrayとnp.dotで簡単に実装できる。まんま一次関数の直線式と変わらない。
## Section2 活性化関数
- 線形の定義は加法性と斉次性を満たすこと。
- 活性化関数で、次の入力の強弱を設定することができる。
- 中間層用の活性化関数は以下
- - ステップ関数　最近はあまり使われていない。線形分離可能な問いしか使えない。
- - シグモイド関数　ステップ関数の連続版のようなもの。勾配消失問題を起こしやすい。
- - ReLU関数　0より大きい入力に対して、恒等写像を返す。勾配消失とスパース化に貢献。

## Section3 出力層
### 中間層と出力層の出力の役割について
- 中間層：閾値の前後で、出力の強弱を調整。
- 出力層：出力の比率はそのままに変換。分類問題においては総和が１となるように変換する。
- 出力層の活性化関数は以下
- - ソフトマックス関数：多クラス分類問題で利用。誤差関数は交差エントロピーを利用
- - 恒等写像：回帰問題で利用。誤差関数は二乗和誤差
- - シグモイド関数：２値分類問題で利用。誤差関数は交差エントロピーを利用
### 確認テスト(2乗和誤差について)
- 二乗和誤差でないと、異符号の誤差が打ち消しあって誤差を表すことができないために、二乗する。係数の1/2は微分計算のためのものであり、本質的ではない。
### 確認テスト(ソフトマックス関数について)
出力は配列。np.exp(x)はスカラーではなくベクトル値。np.sum(np.exp(x))は総和なのでスカラー
### 確認テスト(交差エントロピーについて)
真数部分が０とならないように微小な値を加えておく。
総和の形式だが、d_iは正解のラベルのみ１・その他は０の場合(One-Hot-Vector)、実質的に返すのは正解についての対数値。
## Section4 勾配降下法
深層学習の目的は、精度を高めるパラメータを求めること。そのため結果から、誤差の分だけパラメータを修正する。
学習の際には学習率によって、学習の精度が大きく変わる。
- 学習率が大きすぎる場合：解にたどり着かず、発散する。
- 学習率が小さすぎる場合：学習が遅い。また局所極小解に捕らわれやすい。
入力 - 出力 - 誤差の計算 - パラメータの更新という一つの流れをエポックという。
### 勾配降下法
全サンプルの平均誤差を計算する。
### 確率的勾配降下法
ランダムに抽出したサンプルの誤差を計算。利点は次の三点
- データが冗長な場合の計算コストの削減
- 鞍点への収束を回避しやすい
- オンライン学習が可能。
### ミニバッチ勾配降下法
ランダムに分割したデータの集合に属するサンプルの平均誤差。利点は次
確率的勾配降下法のメリットを損なわず、計算機の計算資源を有効利用できる。このため、CPUを利用したスレッド並列化やGPUを利用したSIMD並列化
### 確認テスト(オンライン学習とは)
データをすべて学習させるのではなく、その都度学習させる。そのため、すべてのデータをそろえる必要がない。
## Section5 誤差逆伝搬
# 深層学習day2
## Section1 勾配消失問題
## Section2 学習率最適化手法
## Section3 過学習
## Section4 畳み込みニューラルネットワークの概念
## Section5 最新のCNN

