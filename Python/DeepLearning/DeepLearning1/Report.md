# 深層学習day1
- ディープラーニングの目的はプログラムでモデルを明示するのではなく、中間層を重ね重みとバイアスを学習させることで自動的にモデルを作成する。
## Section1 入力層～中間層
- 入力層と中間層で一つのユニットと考える。
- 中間層は何層でも重ねることができる。
- 数字で置き換えられるものはどのようなものでも入力として扱うことができる。
- 入力と重みとバイアスの線形結合はPythonでnp.arrayとnp.dotで簡単に実装できる。まんま一次関数の直線式と変わらない。
## Section2 活性化関数
- 線形の定義は加法性と斉次性を満たすこと。
- 活性化関数で、次の入力の強弱を設定することができる。
- 中間層用の活性化関数は以下
- - ステップ関数　最近はあまり使われていない。線形分離可能な問いしか使えない。
- - シグモイド関数　ステップ関数の連続版のようなもの。勾配消失問題を起こしやすい。
- - ReLU関数　0より大きい入力に対して、恒等写像を返す。勾配消失とスパース化に貢献。

## Section3 出力層
### 中間層と出力層の出力の役割について
- 中間層：閾値の前後で、出力の強弱を調整。
- 出力層：出力の比率はそのままに変換。分類問題においては総和が１となるように変換する。
- 出力層の活性化関数は以下
- - ソフトマックス関数：多クラス分類問題で利用。誤差関数は交差エントロピーを利用
- - 恒等写像：回帰問題で利用。誤差関数は二乗和誤差
- - シグモイド関数：２値分類問題で利用。誤差関数は交差エントロピーを利用
### 確認テスト(2乗和誤差について)
- 二乗和誤差でないと、異符号の誤差が打ち消しあって誤差を表すことができないために、二乗する。係数の1/2は微分計算のためのものであり、本質的ではない。
### 確認テスト(ソフトマックス関数について)
出力は配列。np.exp(x)はスカラーではなくベクトル値。np.sum(np.exp(x))は総和なのでスカラー
### 確認テスト(交差エントロピーについて)
真数部分が０とならないように微小な値を加えておく。
総和の形式だが、d_iは正解のラベルのみ１・その他は０の場合(One-Hot-Vector)、実質的に返すのは正解についての対数値。
## Section4 勾配降下法
深層学習の目的は、精度を高めるパラメータを求めること。そのため結果から、誤差の分だけパラメータを修正する。
学習の際には学習率によって、学習の精度が大きく変わる。
- 学習率が大きすぎる場合：解にたどり着かず、発散する。
- 学習率が小さすぎる場合：学習が遅い。また局所極小解に捕らわれやすい。
入力 - 出力 - 誤差の計算 - パラメータの更新という一つの流れをエポックという。
### 勾配降下法
全サンプルの平均誤差を計算する。
### 確率的勾配降下法
ランダムに抽出したサンプルの誤差を計算。利点は次の三点
- データが冗長な場合の計算コストの削減
- 鞍点への収束を回避しやすい
- オンライン学習が可能。
### ミニバッチ勾配降下法
ランダムに分割したデータの集合に属するサンプルの平均誤差。利点は次
確率的勾配降下法のメリットを損なわず、計算機の計算資源を有効利用できる。このため、CPUを利用したスレッド並列化やGPUを利用したSIMD並列化


### 確認テスト(オンライン学習とは)
データをすべて学習させるのではなく、その都度学習させる。そのため、すべてのデータをそろえる必要がない。
## Section5 誤差逆伝搬
## 誤差の計算
数値微分(パラメータに微小な値の加えたものと引いたものの差をとって、微小量の２倍で割る。)を用いる。
しかし入力層から出力層に向けての計算をすべての層の重みに対して行うので、計算量が多くなる。(純伝搬)
そのため、誤差逆伝搬法を使う。
## 誤差逆伝搬とは
産出された誤差を、出力層側から順に微分し、入力層に向かって伝搬していく。各パラメータの微分値を解析的に計算する。そのため、不要な再帰計算を避けることができる。
実現するために微分の連鎖律を用いる。
### 計算速度
処理速度と値段は次の関係。CPU < GPU <FPGA < ASIC(TPU)TPUはクラウドのものを間借りする。
### 入力層の設計
入力層としてとるべきでないデータ
- 欠損値が多いデータ
- 誤差の大きいデータ
- 出力を加工した情報　生のデータから出力を得るニューラルネットワークをエンドtoエンドという。
- 連続性のないデータ
- 無意味な数が割り当てられているデータ
### 欠損値の取り扱い
- ゼロ詰め
- 欠損値を含む集合を除外
- 入力として採用しない。
### 正規化と正則化
正規化は入力を0-1に収める。(入力の最大値で割る。)
正則化は平均0、分散１のガウス分布に収める。
# 深層学習day2
## Section1 勾配消失問題
誤差逆伝搬法が階層に進んでいくにつれて、勾配がどんどん緩やかになる。そのため学習によるパラメータ更新が進まなくなる。
学習しても正解率が上がらない、連鎖律で伝わる結果が0-1の間をとるため、積をとると値が小さくなる。
### 対策1　活性化関数の工夫
- ReLu関数：0以上の時、y=x,0未満の場合は0を返す関数。勾配消失問題およびスパース化に貢献する。
- - この関数により、重み更新に貢献しない重みの更新を行わない。
### 対策2　初期値の設定方法
重みの初期値は乱数によって設定する。
- Xavierの初期値：正規分布をとった後、前のlayerのノードの平方根で割る。S字カーブの活性化関数に対して有効
np.randam.rand(input_layer_size,hidden_layer_size) / np.sqrt(input_layer_size)
np.random.rand()は標準正規分布に沿った乱数を返す。変数の指定で出力される行列のサイズを決定する。
- Heの初期値：正規分布の重みに2/ｎの平方根をかける。nは前層のノード数。ReLu関数に対して有効
np.randam.rand(input_layer_size,hidden_layer_size) / np.sqrt(input_layer_size)* np.sqrt(2)
- 正規分布による初期化：出力値が0または1に集中する。勾配消失が起きやすい
- 正規分布の標準偏差を極端(0.01)に小さくする。出力地が0.5に集中する。表現が失われ学習できていない。

### 対策3　バッチ正規化
### 確認テスト
シグモイド関数は勾配消失問題を起こしやすい。(導関数の最大値が0.25(x=0)。シグモイド関数の導関数はシグモイド関数の2次式なので最大値を容易に求められる。)
## Section2 学習率最適化手法
## Section3 過学習
## Section4 畳み込みニューラルネットワークの概念
## Section5 最新のCNN

