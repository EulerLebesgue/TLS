# 深層学習day3
## Section1 再帰ニューラルネットワーク(RNN)の概念
時系列データに対応可能なニューラルネットワーク。  
時系列データとは：時間的なつながりが統計的依存関係があるもの。音声・テキストなど。  
### 実装について
- 時系列を表すために中間層の出力を次の中間層の入力に使う。
- 重みは大きく分けて3か所。入力から中間層、前の中間層からの次の中間層、中間層から出力層。
- t-1の状態からtの状態を求める(再帰的)
### 構文木
単語同士のつながり(特徴量)を学習させ、最後1つの特徴量を学習する。
### BPTT(RNNにおける逆伝搬)
中間層から出力層の重みとバイアスの更新には時間的なつながりを考慮しなくてよい。これらはすでに入力層から中間層への伝達で反映されているので。  
したがって、中間層から出力層への重みとバイアスの更新は総和を計算する必要がない。
## Section2 LSTM
RNNにおいて、時系列によりレイヤーが増えることから勾配消失・勾配爆発問題が起きやすい・(8回で1セットのデータであれば、データを学習するのに8層あるのと同じ。)
### CEC
LNNの記憶部分を担う箇所。勾配問題において、勾配が1であれば影響がない。重みが一律となるため、学習機能がなくなる。
### 入力ゲートと出力ゲート
- 入力ゲート：CECが入力データをどのように記憶させるかを学習する。
- 出力ゲート：CECが記憶でため込んだ内容をどのように出力するかを学習する。
いずれも、時間tの入力に対する重みと時間t-1の出力に対する出力の重みを学習する。
### 忘却ゲート
過去の情報がいらなくなった場合、そのタイミングで情報を忘却する必要がある。
### 確認テスト
忘却ゲートを利用することで、動詞を予測するのに不要な品詞の影響を抑えることができる。
### 覗き穴結合
各ゲートに対して、現在のCECの状態を使用するもの。ただしあまり精度向上は見られなかった。
## Section3 GRU
LSTMではパラメータ数が多く、計算負荷が高い。そのためパラメータを大幅に削減したもの。深層学習では同じような開発の流れをとる。  
CECが完全になくなった形状。ゲートもリセットゲートと更新ゲートのみとなっている。
リセットゲート：隠れ層でどのように状態を保持するかを決める。
更新ゲート：前回の記憶と今回の記憶の使い方を決める。
### 確認テスト
CECの問題点：学習能力がない。記憶しかない。 GRAにはCECが存在しない、
## Section4 双方向RNN
過去の情報だけでなく、未来の情報を加味することで精度を向上させるモデル。  
過去からの入力と現在の入力への層および未来からの入力と現在の入力の層を合わせたもの。  
情報をつぶさずに考慮するには、concateneteを用いる。  
concateneteはaxisの指定で結合方法はが異なる。axis=0は一次元配列。axis=1では同じ要素で二次元の配列を作るイメージ。  
順方向と逆方向の伝搬情報を合わせる場合、t時間の情報をペアとして持つ必要があるので、axis = 1  
## Section5 Seq2Seq
2つのニューラルネットワークを組み合わせたモデル。時系列データから時系列データを生成する。機械翻訳で使用される。  
1つ目のネットワークでは入力で、文脈の意味を保持する。(Encoder)2つ目のネットワークでは文脈の意味から新しい文章を作成する。(Decoder)
### Encode RNN
インプットしたテキストデータを単語等のトークンに区切って渡す。通常記憶させる単語を限定する。3～5万語程度  
1. 単語ごとにIDを振る(Taking)
2. IDに対してone-hotベクトルで表現する。(列数が単語数の行列)
3. IDから、そのトークンを表す分散表現ベクトルに変換(Embedding)(数百程度)。落とし方はディープラーニングで単語の意味を類似性を抽出することで実現する
4. 得られたベクトルを順番にRNNに入力していく(Encoder RNN)

- MLM Masked Language Model：文脈から1単語を隠し、周辺単語から隠した単語を自力で予測させる。教師なし学習。

### Decode RNN
システムがアウトプットデータを単語等のトークンごとに生成する構造。  
処理をEncodeと逆。Embedding表現から近い単語を予想する。
1. Encoder RNNのfinal stateから各tokenの生成確率を出力する。finalstateをDecoderRNNのinitialStateととして、Embeddingを入力する。(DecoderRNN)
2. 生成確率にもとづいて、tokenをランダムに選択。(Sampling)
3. 選ばれたtokenをEmbeddingしてDecoderRNNへの次の入力とする。(Embedding)
4. 上記処理を繰り返し、得られたtokenを文字列に変換する。(Detokenize)
### 確認テスト
### HRED
### VHRED
### VAE

## Section6 Word2Vec

## Section7 Attention Machinism

# 深層学習day4
## Section1 強化学習
## Section2 AlphaGo
## Section3 軽量化・高速化技術
## Section4 Transformer
## Section5 物体検知・セグメンテーション

