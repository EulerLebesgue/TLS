# 深層学習day3
## Section1 再帰ニューラルネットワーク(RNN)の概念
時系列データに対応可能なニューラルネットワーク。  
時系列データとは：時間的なつながりが統計的依存関係があるもの。音声・テキストなど。  
### 実装について
- 時系列を表すために中間層の出力を次の中間層の入力に使う。
- 重みは大きく分けて3か所。入力から中間層、前の中間層からの次の中間層、中間層から出力層。
- t-1の状態からtの状態を求める(再帰的)
### 構文木
単語同士のつながり(特徴量)を学習させ、最後1つの特徴量を学習する。
### BPTT(RNNにおける逆伝搬)
中間層から出力層の重みとバイアスの更新には時間的なつながりを考慮しなくてよい。これらはすでに入力層から中間層への伝達で反映されているので。  
したがって、中間層から出力層への重みとバイアスの更新は総和を計算する必要がない。
## Section2 LSTM
RNNにおいて、時系列によりレイヤーが増えることから勾配消失・勾配爆発問題が起きやすい・(8回で1セットのデータであれば、データを学習するのに8層あるのと同じ。)
### CEC
LNNの記憶部分を担う箇所。勾配問題において、勾配が1であれば影響がない。重みが一律となるため、学習機能がなくなる。
### 入力ゲートと出力ゲート
- 入力ゲート：CECが入力データをどのように記憶させるかを学習する。
- 出力ゲート：CECが記憶でため込んだ内容をどのように出力するかを学習する。
いずれも、時間tの入力に対する重みと時間t-1の出力に対する出力の重みを学習する。
### 忘却ゲート
過去の情報がいらなくなった場合、そのタイミングで情報を忘却する必要がある。
### 確認テスト
忘却ゲートを利用することで、動詞を予測するのに不要な品詞の影響を抑えることができる。
### 覗き穴結合
各ゲートに対して、現在のCECの状態を使用するもの。ただしあまり精度向上は見られなかった。
## Section3 GRU
LSTMではパラメータ数が多く、計算負荷が高い。そのためパラメータを大幅に削減したもの。深層学習では同じような開発の流れをとる。  
CECが完全になくなった形状。ゲートもリセットゲートと更新ゲートのみとなっている。
リセットゲート：隠れ層でどのように状態を保持するかを決める。
更新ゲート：前回の記憶と今回の記憶の使い方を決める。
### 確認テスト
CECの問題点：学習能力がない。記憶しかない。 GRAにはCECが存在しない、
## Section4 双方向RNN
過去の情報だけでなく、未来の情報を加味することで精度を向上させるモデル。  
過去からの入力と現在の入力への層および未来からの入力と現在の入力の層を合わせたもの。  
情報をつぶさずに考慮するには、concateneteを用いる。  
concateneteはaxisの指定で結合方法はが異なる。axis=0は一次元配列。axis=1では同じ要素で二次元の配列を作るイメージ。  
順方向と逆方向の伝搬情報を合わせる場合、t時間の情報をペアとして持つ必要があるので、axis = 1  
## Section5 Seq2Seq
2つのニューラルネットワークを組み合わせたモデル。時系列データから時系列データを生成する。機械翻訳で使用される。  
1つ目のネットワークでは入力で、文脈の意味を保持する。(Encoder)2つ目のネットワークでは文脈の意味から新しい文章を作成する。(Decoder)
### Encode RNN
インプットしたテキストデータを単語等のトークンに区切って渡す。通常記憶させる単語を限定する。3～5万語程度  
1. 単語ごとにIDを振る(Taking)
2. IDに対してone-hotベクトルで表現する。(列数が単語数の行列)
3. IDから、そのトークンを表す分散表現ベクトルに変換(Embedding)(数百程度)。落とし方はディープラーニングで単語の意味を類似性を抽出することで実現する
4. 得られたベクトルを順番にRNNに入力していく(Encoder RNN)

- MLM Masked Language Model：文脈から1単語を隠し、周辺単語から隠した単語を自力で予測させる。教師なし学習。

### Decode RNN
システムがアウトプットデータを単語等のトークンごとに生成する構造。  
処理をEncodeと逆。Embedding表現から近い単語を予想する。
1. Encoder RNNのfinal stateから各tokenの生成確率を出力する。finalstateをDecoderRNNのinitialStateととして、Embeddingを入力する。(DecoderRNN)
2. 生成確率にもとづいて、tokenをランダムに選択。(Sampling)
3. 選ばれたtokenをEmbeddingしてDecoderRNNへの次の入力とする。(Embedding)
4. 上記処理を繰り返し、得られたtokenを文字列に変換する。(Detokenize)

### HRED
seq2seq課題：１問１答しかできない。問いに対して文脈も何もなく、ただ応答が行われ続ける。  
文脈そのものをtokenのようにつながりがあるものとして扱う。RNNの隠れ層をさらにRNNで接続する。  
seq2seq + Context RNN  
Context RNN:Encoderのまとめた各文章の系列をまとめて、これまでの会話コンテキスト全体を表すベクトルに変換する構造。  
HREDの課題：会話の流れの多様性がない。情報量に乏しい答えを返す。相槌など
### VHRED
VAEの潜在変数の概念を導入。HREDよりも表現力の高いモデルを実現する。
### VAE
#### オートエンコーダー(自己符号化器)
教師なし学習の一つ。Encodeで潜在変数zを見つけ出し、Decoderで入力から同一の出力を得るモデル。  
メリット：次元削減を行うことが可能。
#### VAE
潜在変数zを標準正規分布に従うとする。  
オートエンコーダはDecoderが解釈しやすければなんでもよい。(例えば、10個の数字に対してone-hot-vectorの分類を学んでもよい。)  
確率分布を入れることで類似性を表現できるEncoderが学習を行う。  
実際には学習の際に、エンコードの際にノイズを加える。(ドロップアウト？)ノイズがあるため、Decoderの汎用性が上がるもの
## Section6 Word2Vec
単語のEmbeding表現を得るための手法。  
単語同士の意味を保ちながら、ベクトル表現を得る。  
これにより、大量のone-hotvector表現から、現実的な速度とメモリ量で計算可能な学習を可能とした。  
行列のサイズはボキャブラリ×任意の単語ベクトル次元の重み行列で実現可能。
## Section7 Attention Machinism
seq2seqでは長い文章には対応できない。どんな語数でも固定長ベクトルで表現する必要がある。  
そのため、一文の中で大事な単語を見分けて隠れ層として学習する必要がある。  
（例えば、文章中の冠詞は同紙に比べて重要度が低いなど。 ）

### 確認テスト
- RNN：時系列データを扱うのに適したNN  
- word2vec:単語の分散表現ベクトルを得る手法
- seq2seq：１つの時系列データから別の時系列データを得るNN
- Attention Machinism：時系列データの中身の関連性に重みをつける手法。
# 深層学習day4
## Section1 強化学習
### 強化学習とは
長期的に報酬を最大化できるように環境のなかで行動を選択できるエージェントを作ることを目標とする機械学習の一分野  
行動の結果として与えられる利益をもとに、行動を決定する原理を改善するメカニズム  
近年発達した理由は計算速度の進展と関数近似法とQ学習を組み合わせること。
### 強化学習の応用例
- 環境：会社の販売促進部
- エージェント：キャンペーンメールを送る顧客を決めるソフト
- 行動：顧客ごとに送信、非送信の二つ行動をとる
- 報酬：キャンペーンコストとキャンペーンで生み出される売り上げ
### 探索と利用のトレードオフ
強化学習では環境についての知識はないと仮定する。そのため、ベストの行動を探す(探索)と過去の経験を活かす(利用)がトレードオフとなる。
### 強化学習のイメージ
学習をするのは[方策](報酬がたくさんもらえるように)と[価値]　(もっともよい価値を与える。。)
### 強化学習の差分
教師あり・教師なしは情報の特徴を抽出し予測する。強化学習は、行動の指針を学習する。
### 行動価値関数
価値関数には2つある  
- 状態価値関数：ある状態の価値に注目。エージェントの状態は関係ない。
- 行動価値関数：状態と価値を組み合わせた価値に注目
### 方策関数
ある状態でどのような行動を採るのかの確率を与える関数。行動価値関数が最大となるように決定する。
### 方策勾配法
方策をモデル化して最適化する手法。NNのW(重みに当たるものを学習) Θ_(t+1) = Θ_t + e * ∇J(Θ) 報酬が大きくなるようにしたいので+  
Jとは方策の良さ（期待報酬）を返す（NNの誤差関数）。これは定義を行う。定義方法は下記  
- 平均報酬
- 割引報酬和
## Section2 AlphaGo
2つの畳み込みニューラルネットワークからできている。
### PolicyNet（状態価値関数）
- - 盤面特徴入力は19×19 48チャンネル
畳み込みで中間層の活性化関数はReLU関数。出力層の活性化関数はSoftMax関数(二次元盤面の確率を出す。)
#### PolicyNetの学習
- 現状のPolicyNet VS PolicyPoolからランダムに選択したPolicyNetで対局を行う。PlicyNetとは強化学習の過程を500イテレーションごとに記録し保存したもの
- これにより過学習を防ぐことができる。mini batch size 128で1万回実施
### ValueNet（行動価値関数）
- - 盤面特徴入力は19×19 49チャンネル。PolicyNetより手番を考慮するので1チャンネル増える。
畳み込みで中間層の活性化関数はReLU関数。全結合は2回かませて、出力層の活性化関数はTanH関数(勝ち負けの-1から1の1次元データ)
#### ValueNetの学習
- PolicyNetを使用して対局シュミレーションした結果の勝敗を教師として学習。
1. 教師あり学習で作成したPolicyNet(SL PolicyNet)でN手まで打つ
2. N+1手目をランダムに選択し、その手で進めた局面をS(N+1)とする。
3. S(N+1)から強化学習で作成したPolicyNet(RL PolicyNet)で終局まで打ち、その勝敗報酬をRとする。ここでPolicyNetを別とするのは過学習を防ぐため。
4. S(N+1)とRを教師データ対として、平均二乗誤差を損失関数として回帰問題として学習した
mini batch size 32で5000万回学習する。
## Section3 軽量化・高速化技術
### モデル並列
### データ並列
### GPU
### 量子化
### 蒸留
### プーリング

## Section4 Transformer
## Section5 物体検知・セグメンテーション

